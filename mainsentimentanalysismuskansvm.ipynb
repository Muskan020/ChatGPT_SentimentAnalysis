{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6577397,"sourceType":"datasetVersion","datasetId":3798337}],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_curve, auc\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2023-10-02T06:34:15.914247Z","iopub.execute_input":"2023-10-02T06:34:15.914488Z","iopub.status.idle":"2023-10-02T06:34:25.182034Z","shell.execute_reply.started":"2023-10-02T06:34:15.914463Z","shell.execute_reply":"2023-10-02T06:34:25.181075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport pandas as pd\n\n# Try different encodings\nencodings_to_try = ['utf-8', 'latin1', 'iso-8859-1', 'utf-16']\n\n# Attempt to read the CSV file using different encodings\nfor encoding in encodings_to_try:\n    try:\n        data = pd.read_csv(\"/kaggle/input/dataset/dataChatGPT.csv\", encoding=encoding)\n        break  # If successful, stop trying other encodings\n    except UnicodeDecodeError:\n        continue  # Try the next encoding if decoding fails\n\n# Drop the first column\ndata = data.drop(data.columns[0], axis=1)\n\n# Print the first 5 rows as a list\nprint(data.values[:5].tolist())","metadata":{"execution":{"iopub.status.busy":"2023-10-02T06:34:25.183699Z","iopub.execute_input":"2023-10-02T06:34:25.184528Z","iopub.status.idle":"2023-10-02T06:34:29.313923Z","shell.execute_reply.started":"2023-10-02T06:34:25.184494Z","shell.execute_reply":"2023-10-02T06:34:29.312950Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2023-10-02T06:34:29.315507Z","iopub.execute_input":"2023-10-02T06:34:29.316159Z","iopub.status.idle":"2023-10-02T06:34:29.329251Z","shell.execute_reply.started":"2023-10-02T06:34:29.316124Z","shell.execute_reply":"2023-10-02T06:34:29.328067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from wordcloud import WordCloud\n\ntext_data = data['tweets'].str.cat(sep=' ')\n\n# Create a WordCloud object\nwordcloud = WordCloud(width=800, height=400, background_color='#F5F5F5').generate(text_data)\n\n# Display the word cloud using Matplotlib\nplt.figure(figsize=(10, 5))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-02T06:34:29.332049Z","iopub.execute_input":"2023-10-02T06:34:29.332956Z","iopub.status.idle":"2023-10-02T06:35:35.996769Z","shell.execute_reply.started":"2023-10-02T06:34:29.332922Z","shell.execute_reply":"2023-10-02T06:35:35.995973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Duplicated values: \" , data.duplicated().sum())\n\ndata.drop_duplicates(inplace=True)\ndata = data.dropna(axis=0)\nprint(data.info())","metadata":{"execution":{"iopub.status.busy":"2023-10-02T06:35:35.997659Z","iopub.execute_input":"2023-10-02T06:35:35.997968Z","iopub.status.idle":"2023-10-02T06:35:37.280293Z","shell.execute_reply.started":"2023-10-02T06:35:35.997937Z","shell.execute_reply":"2023-10-02T06:35:37.279340Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[\"labels\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-10-02T06:35:37.281452Z","iopub.execute_input":"2023-10-02T06:35:37.282402Z","iopub.status.idle":"2023-10-02T06:35:37.334281Z","shell.execute_reply.started":"2023-10-02T06:35:37.282367Z","shell.execute_reply":"2023-10-02T06:35:37.333332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_0 = data[data[\"labels\"] == \"neutral\"]\nclass_1 = data[data[\"labels\"] == \"good\"]\nclass_neg = data[data[\"labels\"] == \"bad\"]\nclass_neg  = class_neg.iloc[:70000]\ndata = pd.concat([class_neg , class_0 , class_1])\n\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2023-10-02T06:35:37.335671Z","iopub.execute_input":"2023-10-02T06:35:37.336210Z","iopub.status.idle":"2023-10-02T06:35:37.550200Z","shell.execute_reply.started":"2023-10-02T06:35:37.336178Z","shell.execute_reply":"2023-10-02T06:35:37.549325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[\"labels\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-10-02T06:35:37.551821Z","iopub.execute_input":"2023-10-02T06:35:37.552663Z","iopub.status.idle":"2023-10-02T06:35:37.597067Z","shell.execute_reply.started":"2023-10-02T06:35:37.552588Z","shell.execute_reply":"2023-10-02T06:35:37.596154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/","metadata":{"execution":{"iopub.status.busy":"2023-10-02T06:35:37.598440Z","iopub.execute_input":"2023-10-02T06:35:37.599000Z","iopub.status.idle":"2023-10-02T06:35:38.908227Z","shell.execute_reply.started":"2023-10-02T06:35:37.598966Z","shell.execute_reply":"2023-10-02T06:35:38.907118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import wordnet as wn\nfrom nltk.stem import WordNetLemmatizer, PorterStemmer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\n\n# Download necessary NLTK resources\nnltk.download('wordnet')\nnltk.download('punkt')\nnltk.download('stopwords')\n\n# Initialize stopwords, stemmer, and lemmatizer\nstop_words = set(stopwords.words('english'))\nst = PorterStemmer()\nlem = WordNetLemmatizer()\n\ndef is_alpha(word):\n    for part in word.split('-'):\n        if not part.isalpha():\n            return False\n    \n    return True\n\ndef clean_dataset(text):\n    text = re.sub(r'http\\S+', '', text)  # Removing links\n    text = re.sub(r'\\\\n', ' ', text)     # Removing \\\\n\n    text = re.sub(r\"\\s*#\\S+\", \"\", text)   # Removing hash tags\n    text = re.sub(r\"\\s*@\\S+\", \"\", text)   # Removing @\n    text = text.lower()\n    words = [word for word in word_tokenize(text) if is_alpha(word)]\n    words = [lem.lemmatize(word) for word in words if word not in stop_words]\n    text = \" \".join(words)\n    \n    return text.strip()\n\n# Assuming 'data' is defined and 'tweets' column exists\ndata.insert(len(data.columns) - 1, \"cleaned_tweets\", data['tweets'].apply(clean_dataset))\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T06:35:38.912743Z","iopub.execute_input":"2023-10-02T06:35:38.913026Z","iopub.status.idle":"2023-10-02T06:38:22.746944Z","shell.execute_reply.started":"2023-10-02T06:35:38.913000Z","shell.execute_reply":"2023-10-02T06:38:22.745953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from wordcloud import WordCloud\n\ntext_data = data['cleaned_tweets'].str.cat(sep=' ')\n\n# Create a WordCloud object\nwordcloud = WordCloud(width=800, height=400, background_color='#F5F5F5').generate(text_data)\n\n# Display the word cloud using Matplotlib\nplt.figure(figsize=(10, 5))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-02T06:38:22.748506Z","iopub.execute_input":"2023-10-02T06:38:22.748877Z","iopub.status.idle":"2023-10-02T06:38:56.229328Z","shell.execute_reply.started":"2023-10-02T06:38:22.748843Z","shell.execute_reply":"2023-10-02T06:38:56.228538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from numpy.core import numeric\n#@title ***Converting text to numerical form***\n\nimport math\nimport collections\n\ndef convert_text_to_numerical(text):\n    num_words = 7000\n    tokenizer = Tokenizer(num_words=num_words)\n    tokenizer.fit_on_texts(text)\n    sequences = tokenizer.texts_to_sequences(text)\n \n    #maxlen = max(45, math.ceil(np.average([len(seq) for seq in sequences])))\n    maxlen = 140\n    \n \n    pad_seqs = pad_sequences(sequences, maxlen=maxlen)\n    pad_seqs_todrop = []\n    for i, p in enumerate(pad_seqs):\n        if sum(p) == sum(sorted(p, reverse=True)[0:2]):\n            pad_seqs_todrop.append(i)\n\n    return pad_seqs, pad_seqs_todrop, tokenizer, num_words, maxlen\n\ndata = data.reset_index()\nnumeric_tweets, rows_todrop, tokenizer, num_words, maxlen = convert_text_to_numerical(data['cleaned_tweets'])\ndata.insert(len(data.columns)-1, \"numeric_tweets\", numeric_tweets.tolist())\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2023-10-02T06:38:56.231887Z","iopub.execute_input":"2023-10-02T06:38:56.232499Z","iopub.status.idle":"2023-10-02T06:39:39.148354Z","shell.execute_reply.started":"2023-10-02T06:38:56.232459Z","shell.execute_reply":"2023-10-02T06:39:39.147445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@title ***Encoding output labels***\n\nlabel_encoder = LabelEncoder()\ndata.insert(len(data.columns), \"encoded_labels\", label_encoder.fit_transform(data['labels']))\nprint(label_encoder.classes_)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2023-10-02T06:39:39.149893Z","iopub.execute_input":"2023-10-02T06:39:39.150535Z","iopub.status.idle":"2023-10-02T06:39:39.269793Z","shell.execute_reply.started":"2023-10-02T06:39:39.150502Z","shell.execute_reply":"2023-10-02T06:39:39.268858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_data = data.drop(rows_todrop, inplace=False)\nprint(len(data), len(final_data))\nfinal_data.head()","metadata":{"execution":{"iopub.status.busy":"2023-10-02T06:39:39.270979Z","iopub.execute_input":"2023-10-02T06:39:39.271887Z","iopub.status.idle":"2023-10-02T06:39:39.363567Z","shell.execute_reply.started":"2023-10-02T06:39:39.271846Z","shell.execute_reply":"2023-10-02T06:39:39.362626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@title ***Splitting the dataset into training and testing sets***\ninputs = final_data[['tweets', 'cleaned_tweets', 'numeric_tweets']]\noutputs = final_data[['labels', 'encoded_labels']]\n\nin_train, in_test, out_train, out_test = train_test_split(inputs, outputs, test_size=0.2, shuffle=True, random_state=42)\n\nX_train = in_train['numeric_tweets']\nX_test = in_test['numeric_tweets'] \ny_train = out_train['encoded_labels']\ny_test = out_test['encoded_labels']","metadata":{"execution":{"iopub.status.busy":"2023-10-02T06:39:39.365263Z","iopub.execute_input":"2023-10-02T06:39:39.365951Z","iopub.status.idle":"2023-10-02T06:39:39.686243Z","shell.execute_reply.started":"2023-10-02T06:39:39.365915Z","shell.execute_reply":"2023-10-02T06:39:39.685293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = np.asarray(X_train.tolist())\nX_train = X_train.astype(np.int32)\nX_test = np.asarray(X_test.tolist()).astype(np.int32)\ny_train = np.asarray(y_train.tolist()).astype(np.int32)\ny_test = np.asarray(y_test.tolist()).astype(np.int32)\ntype(X_train)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T06:39:39.687711Z","iopub.execute_input":"2023-10-02T06:39:39.688277Z","iopub.status.idle":"2023-10-02T06:39:47.070281Z","shell.execute_reply.started":"2023-10-02T06:39:39.688243Z","shell.execute_reply":"2023-10-02T06:39:47.069381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train","metadata":{"execution":{"iopub.status.busy":"2023-10-02T06:39:47.071756Z","iopub.execute_input":"2023-10-02T06:39:47.072707Z","iopub.status.idle":"2023-10-02T06:39:47.079306Z","shell.execute_reply.started":"2023-10-02T06:39:47.072674Z","shell.execute_reply":"2023-10-02T06:39:47.078357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report, accuracy_score","metadata":{"execution":{"iopub.status.busy":"2023-10-02T06:41:07.257179Z","iopub.execute_input":"2023-10-02T06:41:07.257564Z","iopub.status.idle":"2023-10-02T06:41:07.262959Z","shell.execute_reply.started":"2023-10-02T06:41:07.257535Z","shell.execute_reply":"2023-10-02T06:41:07.261897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming X_train and X_test are NumPy arrays of text data\n# Convert the text data to lowercase using a list comprehension\nX_train = [str(text).lower() for text in X_train]\nX_test = [str(text).lower() for text in X_test]\n\n\n# TF-IDF Vectorization\ntfidf_vectorizer = TfidfVectorizer(max_features=5000)  # You can adjust max_features as needed\nX_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\nX_test_tfidf = tfidf_vectorizer.transform(X_test)\n\n# Train an SVM classifier\nsvm_classifier = SVC(kernel='linear', C=1.0)\nsvm_classifier.fit(X_train_tfidf, y_train)\n\n# Make predictions on the test set\ny_pred = svm_classifier.predict(X_test_tfidf)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nclassification_report = classification_report(y_test, y_pred)\n\nprint(f\"Accuracy: {accuracy}\")\nprint(\"Classification Report:\\n\", classification_report)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T06:41:25.409148Z","iopub.execute_input":"2023-10-02T06:41:25.409479Z"},"trusted":true},"execution_count":null,"outputs":[]}]}